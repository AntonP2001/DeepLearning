# -*- coding: utf-8 -*-
"""Лабораторная работа №3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KjcVLGqB8GqcH1Npx_ElVqh4QGE-ovIY
"""

import numpy as np

class MLP:
    def __init__(self, inputSize, outputSize, learning_rate=0.1, hiddenSizes = 5, hiddenCount = 1):
        self.hiddenCount = hiddenCount
        self.weights = list()

        self.weights.append(np.random.uniform(-2, 2, size=(inputSize,hiddenSizes)))  # веса скрытого слоя)

        for i in range(self.hiddenCount):
          self.weights.append(np.random.uniform(-2, 2, size=(hiddenSizes,hiddenSizes))) # веса выходного слоя)

        self.weights.append(np.random.uniform(-2, 2, size=(hiddenSizes,outputSize))) # веса выходного слоя)

        self.learning_rate = learning_rate
        self.layers = None

    # сигмоида
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def ReLU(self, x):
        y = np.where(x > 0, x, 0)
        return y

    def derivative_ReLU(self, x):
        return x

    # нам понадобится производная от сигмоиды при вычислении градиента
    def derivative_sigmoid(self, x):
        return self.sigmoid(x) * (1 - self.sigmoid(x))

    # прямой проход
    def feed_forward(self, x):
        layers = list()
        input_ = x # входные сигналы
        layers.append(input_)

        for i in range(self.hiddenCount):
          layers.append(self.ReLU(np.dot(layers[-1], self.weights[i]))) # выход скрытого слоя = сигмоида(входные сигналы*веса скрытого слоя)

        output_ = self.ReLU(np.dot(layers[-1], self.weights[-1]))# выход сети (последнего слоя) = сигмоида(выход скрытого слоя*веса выходного слоя)

        self.layers = layers
        return self.layers[-1]


    # backprop собственной персоной
    # на вход принимает скорость обучения, реальные ответы, предсказанные сетью ответы и выходы всех слоев после прямого прохода
    def backward(self, target):

        # считаем производную ошибки сети
        err = (target - self.layers[-1])

        # прогоняем производную ошибки обратно ко входу, считая градиенты и корректируя веса
        # для этого используем chain rule
        # цикл перебирает слои от последнего к первому
        for i in range(len(self.layers)-1, 0, -1):
            # градиент слоя = ошибка слоя * производную функции активации * на входные сигналы слоя

            # ошибка слоя * производную функции активации
            err_delta = err * self.derivative_ReLU(self.layers[i])

            # пробрасываем ошибку на предыдущий слой
            err = np.dot(err_delta, self.weights[i - 1].T)

            # ошибка слоя * производную функции активации * на входные сигналы слоя
            dw = np.dot(self.layers[i - 1].T, err_delta)

            # обновляем веса слоя
            self.weights[i - 1] += self.learning_rate * dw



    # функция обучения чередует прямой и обратный проход
    def train(self, x_values, target):
        self.feed_forward(x_values)
        self.backward(target)

    # функция предсказания возвращает только выход последнего слоя
    def predict(self, x_values):
        return self.feed_forward(x_values)

import pandas as pd
import numpy as np

df = pd.read_csv('/content/drive/MyDrive/Глубокое обучение/data.csv')
df = df.iloc[np.random.permutation(len(df))]

X = df.iloc[0:100, 0:3].values
y = df.iloc[0:100, 4]
y = y.map({'Iris-setosa': 1, 'Iris-virginica': 2, 'Iris-versicolor':3}).values.reshape(-1,1)
Y = np.zeros((y.shape[0], np.unique(y).shape[0]))
for i in np.unique(y):
    Y[:,i-1] = np.where(y == i, 1, 0).reshape(1,-1)

X_test = df.iloc[100:150, 0:3].values
y = df.iloc[100:150, 4]
y = y.map({'Iris-setosa': 1, 'Iris-virginica': 2, 'Iris-versicolor':3}).values.reshape(-1,1)
Y_test = np.zeros((y.shape[0], np.unique(y).shape[0]))
for i in np.unique(y):
    Y_test[:,i-1] = np.where(y == i, 1, 0).reshape(1,-1)

inputSize = X.shape[1] # количество входных сигналов равно количеству признаков задачи
hiddenSizes = 2 # задаем число нейронов скрытого слоя
outputSize = Y.shape[1] if not len(Y.shape) else 1 # количество выходных сигналов равно количеству классов задачи

net = MLP(inputSize, outputSize, hiddenSizes=hiddenSizes, learning_rate=0.06, hiddenCount=2);

for i in range(350):
  net.train(X_test, y)

pr = net.predict(X_test)[-1]
print('Ошибка на обучающей выборке:', sum(abs(y-(pr>0.5))))